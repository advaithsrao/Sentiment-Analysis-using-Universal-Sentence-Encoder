{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFqTDjSPvE6W"
   },
   "source": [
    "\n",
    "## What is sentiment analysis?\n",
    "\n",
    "####  Sentiment Analysis is the process of ‘computationally’ determining whether a document(sentence, phrase) is positive, negative or neutral. It’s also known as opinion mining, deriving the opinion or attitude of a speaker.\n",
    "\n",
    "## Why sentiment analysis?\n",
    "\n",
    "1. **Business:** In marketing field companies use it to develop their strategies, to understand customers’ feelings towards products or brand, how people respond to their campaigns or product launches and why consumers don’t buy some products\n",
    "2. **Politics:** In political field, it is used to keep track of political view, to detect consistency and inconsistency between statements and actions at the government level. It can be used to predict election results as well!\n",
    "3. **Public Actions:** Sentiment analysis also is used to monitor and analyse social phenomena, for the spotting of potentially dangerous situations and determining the general mood of the blogosphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVFvr6VevMqp",
    "outputId": "21754ad0-6dd2-4cbe-e6bd-e0a079dfb79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
      "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.40.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.8.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow_hub==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub==0.7.0) (3.17.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub==0.7.0) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub==0.7.0) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas imblearn matplotlib nltk scikit-learn seaborn\n",
    "!pip install \"tensorflow==1.15.0\"\n",
    "!pip install \"tensorflow_hub==0.7.0\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#try tunning on a GPU accelerated run time\n",
    "# !pip3 install tensorflow_text==1.15\n",
    "# !pip install tensorflow_text\n",
    "# import tensorflow_text\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from nltk import sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br2BXfo9vE6c"
   },
   "source": [
    "## The data file attached is a set of tweets\n",
    "\n",
    "#### **Source:** https://www.kaggle.com/kazanova/sentiment140\n",
    "\n",
    "#### Since the source file is >100MB let us try to implement this with a subset (found in data folder) \n",
    "#### Kindly extract training.1600000.processed.noemoticon.csv.zip and uncomment the lines in the next cell to execute the analysis for all the tweets(80k positive + 80k negative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Rej6F4amvMqq",
    "outputId": "93009d12-9977-42d1-9e77-7aa302c9a829"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Importing the dataset\n",
    "# DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "# DATASET_ENCODING = \"ISO-8859-1\"\n",
    "# df = pd.read_csv('data/training.1600000.processed.noemoticon.csv',\n",
    "#                       encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "\n",
    "# # Removing the unnecessary columns.\n",
    "# df = df[['text','sentiment']]\n",
    "# df.columns =['tweet','label'] \n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kOZ8nMbXvMqr"
   },
   "outputs": [],
   "source": [
    "# # let us down sample this to save execution time and keep ~20k examples per target\n",
    "# df = pd.concat([df[df.label==0].iloc[:20000,:],df[df.label==4].iloc[:20000,:]]).sample(frac=1).reset_index(drop=True)\n",
    "# df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Al6lptC7vMqr",
    "outputId": "8603c131-c073-4307-ba41-b5926f58ec3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    20000\n",
       "0    20000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/data/tweets.csv\")\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hcYc7viAvE6e"
   },
   "outputs": [],
   "source": [
    "# #check if there is null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdCB15-0vMqs"
   },
   "source": [
    "#### since there are two columns '0' and '1', let us rename them to 'negative' and 'positive' respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "VYYwKbHwvMqt",
    "outputId": "fb47fbfd-2c74-45ae-8e33-25e835ecf8cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f222bfb77d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.label = df.label.map({0:'negative',4:'positive'})\n",
    "sns.countplot(x='label', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr3loCKhvMqu"
   },
   "source": [
    "#### Let us call the preprocess function on the text column, to clean the text by,\n",
    "1. Removing non-alphanumeric characters and replace emojis with words\n",
    "2. Lowercase text\n",
    "3. Stemming Words\n",
    "4. Lemmatizing words to its 'lemma' form\n",
    "5. Removing Stop World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bEuG8zQEvMqv"
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# all_stopwords = stopwords.words('english')\n",
    "# all_stopwords.remove('not')\n",
    "stemmer = PorterStemmer()\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.split()\n",
    "    text = [word if word not in emojis.keys() else emojis[word] for word in text]\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VOiCxY9RvMqv"
   },
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TvjTt8B7xlwX"
   },
   "outputs": [],
   "source": [
    "# X_train = df[df.split=='train'].tweet.tolist()\n",
    "# y_train = df[df.split=='train'].label.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"tweet\"], df[\"label\"],\n",
    "                                                    stratify=df[\"label\"],\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=22)\n",
    "\n",
    "# X_test = df[df.split=='test'].tweet.tolist()\n",
    "# y_test = df[df.split=='test'].label.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Omq9w1RXDTTE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6TYFHwqfeVL"
   },
   "source": [
    "## Let us try out the following techniques to analyze the sentiment in these tweets\n",
    "1. Universal Sentence Encoder with SVM Gaussian\n",
    "2. Universal Sentence Encoder with Naive Bayes\n",
    "3. TFIDF with SVM Gaussian\n",
    "4. TFIDF with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xtFcDh0vMqw"
   },
   "source": [
    "## Universal Sentence Encoder\n",
    "#### source : https://aclanthology.org/D18-2029/\n",
    "#### hub : https://tfhub.dev/google/collections/universal-sentence-encoder/1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XoOBg1pvMqx",
    "outputId": "c98c7d16-10c9-4a2b-f73e-0c7aacbd26ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "## Load USE\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module('https://tfhub.dev/google/universal-sentence-encoder/2')\n",
    "\n",
    "with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        X_train_use = session.run(embed(X_train))\n",
    "        X_test_use = session.run(embed(X_test))\n",
    "\n",
    "\n",
    "# with tf.device('/gpu:0'):\n",
    "#           train_embed = embed(X_train)\n",
    "#           test_embeds = embed(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "x8W0BsXiEZK9"
   },
   "outputs": [],
   "source": [
    "# oversample = SMOTE()\n",
    "# X_train_use, y_train_use = oversample.fit_resample(X_train_use, y_train)\n",
    "# summarize the new class distribution\n",
    "# counter = Counter(y_train_use)\n",
    "# print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PFMis_76Vhk",
    "outputId": "e623bcae-76c0-404e-8110-16a96e1a89a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 512)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8Y_TQ90vMqy",
    "outputId": "a1f574e3-bf7d-44d6-c470-5f2f1e4254c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** SVM GAUSSIAN KERNEL WITH UNIVERSAL SENTENCE ENCODER RESULTS *****\n",
      "Accuracy :  0.759\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.76      0.76      3938\n",
      "    positive       0.77      0.76      0.76      4062\n",
      "\n",
      "    accuracy                           0.76      8000\n",
      "   macro avg       0.76      0.76      0.76      8000\n",
      "weighted avg       0.76      0.76      0.76      8000\n",
      "\n",
      "Confusion Matrix:  [[3005  933]\n",
      " [ 995 3067]]\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf',C=5,gamma=5,class_weight = 'balanced') #Call the classifier method\n",
    "svclassifier.fit(X_train_use, y_train) #Fit the model on the training data\n",
    "y_pred = svclassifier.predict(X_test_use) #Predict on the test inputs\n",
    "\n",
    "print(\"***** SVM GAUSSIAN KERNEL WITH UNIVERSAL SENTENCE ENCODER RESULTS *****\")\n",
    "print(\"Accuracy : \",accuracy_score(y_pred,y_test))\n",
    "print(\"Classification Report: \",classification_report(y_pred,y_test))\n",
    "print(\"Confusion Matrix: \",confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2gT3TCm-xVs"
   },
   "source": [
    "#### now let us try to put the sentence embeddings through a Naive Bayes Gaussian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhEubkDNvMqy",
    "outputId": "9aa0593f-f36d-42ae-e1e4-6c2207d560e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** GAUSSIAN NB WITH UNIVERSAL SENTENCE ENCODER RESULTS *****\n",
      "Accuracy :  0.710875\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.72      0.70      3809\n",
      "    positive       0.73      0.70      0.72      4191\n",
      "\n",
      "    accuracy                           0.71      8000\n",
      "   macro avg       0.71      0.71      0.71      8000\n",
      "weighted avg       0.71      0.71      0.71      8000\n",
      "\n",
      "Confusion Matrix:  [[2748 1061]\n",
      " [1252 2939]]\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB() #Call the classifier method\n",
    "nb.fit(X_train_use, y_train) #Fit the model on the training data\n",
    "y_pred = nb.predict(X_test_use) #Predict on the test inputs\n",
    "\n",
    "print(\"***** GAUSSIAN NB WITH UNIVERSAL SENTENCE ENCODER RESULTS *****\")\n",
    "print(\"Accuracy : \",accuracy_score(y_pred,y_test))\n",
    "print(\"Classification Report: \",classification_report(y_pred,y_test))\n",
    "print(\"Confusion Matrix: \",confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aQ_UjIEvE6k"
   },
   "source": [
    "#### So far, we were able to achieve the best overall f1 score with the SVM classifier.\n",
    "#### We could try comparing these results against a few word embedding techniques to see, if using sentence embeddings outperform them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tupS3p_vE6k"
   },
   "source": [
    "## Now let us see how word embeddings perform \n",
    "## We could go with word2vec to embed these tweets\n",
    "### We shall average word2vec embeddings for each work in a tweet to get the embedding for a whole sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sZiifUNKcgVZ"
   },
   "outputs": [],
   "source": [
    "w2v_model = word2vec.Word2Vec(X_train, size=256, min_count=1, iter=50) \n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "w2vX_train = averaged_word_vectorizer(corpus=X_train, model=w2v_model,\n",
    "                                             num_features = 256)\n",
    "w2vX_test = averaged_word_vectorizer(corpus=X_test, model=w2v_model,\n",
    "                                             num_features = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGYT1Y0q_CO3",
    "outputId": "c1b56c27-18dc-4f71-dcaa-14dd9f4d91d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** SVM GAUSSIAN KERNEL WITH TFIDF RESULTS *****\n",
      "Accuracy :  0.587\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.58      0.60      4228\n",
      "    positive       0.56      0.59      0.57      3772\n",
      "\n",
      "    accuracy                           0.59      8000\n",
      "   macro avg       0.59      0.59      0.59      8000\n",
      "weighted avg       0.59      0.59      0.59      8000\n",
      "\n",
      "Confusion Matrix:  [[2462 1766]\n",
      " [1538 2234]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svclassifier = SVC(kernel='rbf',C=5,gamma=5,class_weight = 'balanced') #Call the classifier method\n",
    "svclassifier.fit(w2vX_train, y_train) #Fit the model on the training data\n",
    "y_pred = svclassifier.predict(w2vX_test) #Predict on the test inputs\n",
    "\n",
    "print(\"***** SVM GAUSSIAN KERNEL WITH TFIDF RESULTS *****\")\n",
    "print(\"Accuracy : \",accuracy_score(y_pred,y_test))\n",
    "print(\"Classification Report: \",classification_report(y_pred,y_test))\n",
    "print(\"Confusion Matrix: \",confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfWJJQ9HvE6l",
    "outputId": "44cbf497-a911-437b-b81e-08175414a97f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****GAUSSIAN NB KERNEL WITH TFIDF RESULTS *****\n",
      "Accuracy :  0.53775\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.53      0.60      5196\n",
      "    positive       0.39      0.55      0.46      2804\n",
      "\n",
      "    accuracy                           0.54      8000\n",
      "   macro avg       0.54      0.54      0.53      8000\n",
      "weighted avg       0.58      0.54      0.55      8000\n",
      "\n",
      "Confusion Matrix:  [[2749 2447]\n",
      " [1251 1553]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nb = GaussianNB()\n",
    "nb.fit(w2vX_train, y_train) #Fit the model on the training data\n",
    "y_pred = nb.predict(w2vX_test) #Predict on the test inputs\n",
    "\n",
    "print(\"*****GAUSSIAN NB KERNEL WITH TFIDF RESULTS *****\")\n",
    "print(\"Accuracy : \",accuracy_score(y_pred,y_test))\n",
    "print(\"Classification Report: \",classification_report(y_pred,y_test))\n",
    "print(\"Confusion Matrix: \",confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyBj-i8nfK9O"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "### Further works:\n",
    "\n",
    "1. To improve the performance, we could use more features like Textblob sentence polarity.\n",
    "2. Word2Vec offers features like Negative Emotionality, that could be added in as feature.\n",
    "3. Try out other word embedding techniques and/or frequency based technique like the TFIDF and count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
